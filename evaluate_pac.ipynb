{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Evaluation approach currently used in the script:\n",
    "\n",
    "1. Take the words from test dataset present in the training data too\n",
    "2. Find the approriate implication from the set of learnt implications\n",
    "3. Apply the most common set of operation from the premise of appropriate impl.\n",
    "4. Compare the resultant with the one given in test set.\n",
    "\"\"\"\n",
    "\n",
    "import copy\n",
    "import helper\n",
    "import operator\n",
    "import pandas as pd\n",
    "import concept_context as cn\n",
    "from IPython.display import display\n",
    "\n",
    "TRAIN_DIR = 'data/train/'\n",
    "DEV_DIR = 'data/dev/'\n",
    "COV_TEST_DIR = 'data/test/covered/'\n",
    "UNCOV_TEST_DIR = 'data/test/uncovered/'\n",
    "\n",
    "\n",
    "def evaluate(train_dir, test_dir):\n",
    "    \"\"\"\n",
    "    Parameters:\n",
    "    -----------------\n",
    "    train_dir (Str): Path to the training file\n",
    "    test_dir (Str): Path to the testing file\n",
    "    \"\"\"\n",
    "\n",
    "    # Load training and testing data into a dataframe\n",
    "    train_data = pd.read_csv(train_dir, sep='\\t', names=['source', 'target',\n",
    "                                                         'pos_info'])\n",
    "    test_data = pd.read_csv(test_dir, sep='\\t', names=['source', 'target',\n",
    "                                                         'pos_info'])\n",
    "\n",
    "    attribute_size = train_data['source'].size\n",
    "\n",
    "    test_data = pd.merge(train_data, test_data, how='inner', on=['source',\n",
    "                                                                'target'])\n",
    "    test_data.dropna(inplace=True)\n",
    "    common_words = test_data['source']\n",
    "\n",
    "    # process training data\n",
    "    train_data = train_data.apply(helper.iterLCS, axis=1)\n",
    "\n",
    "    relations = build_relations(train_data)\n",
    "\n",
    "    # Build the concept lattice\n",
    "    concepts = cn.formalConcepts(relations)\n",
    "    concepts.computeLattice()\n",
    "\n",
    "    # Find canonical basis\n",
    "    concepts.computeCanonicalBasis(epsilon=0.1, delta=0.1, basis_type='pac')\n",
    "\n",
    "    print(\"Total implications: {}\\n\".format(len(concepts.canonical_basis)))\n",
    "\n",
    "    unique_conclusions = []\n",
    "    for impl in copy.deepcopy(concepts.canonical_basis):\n",
    "        if len(impl.conclusion) == attribute_size or len(impl.premise) == 0\\\n",
    "            or impl.premise == impl.conclusion:\n",
    "            concepts.canonical_basis.remove(impl)\n",
    "            continue\n",
    "        unique_conclusions.append(frozenset(impl.conclusion))\n",
    "\n",
    "    print(\"Total UNIQUE conclusions: {}\\n\".format(len(set(unique_conclusions))))\n",
    "    concepts.canonical_basis = set(sorted(list(concepts.canonical_basis), reverse=True))\n",
    "\n",
    "    implId_opnSeq_map = {}\n",
    "    for idx, impl in enumerate(concepts.canonical_basis):\n",
    "        premise = train_data['source'].isin(impl.premise)\n",
    "        premise_data = train_data[premise]\n",
    "        implId_opnSeq_map[idx] = operation(premise_data)\n",
    "\n",
    "    word_map = {}\n",
    "    correct = 0\n",
    "    for word in common_words:\n",
    "        # gt => Ground Truth\n",
    "        word_map[word] = {'gt': test_data[test_data['source'] == word]['target'].iloc[0]}\n",
    "        for idx, impl in enumerate(concepts.canonical_basis):\n",
    "            # use conclusion as it contains elements of premise too\n",
    "            if word in impl.conclusion:\n",
    "                opn_seq = implId_opnSeq_map[idx].split(' ')\n",
    "                output = apply_operation(opn_seq, word)\n",
    "                word_map[word]['pac_output'] = output\n",
    "                if word_map[word]['gt'] == output:\n",
    "                    correct += 1\n",
    "                # stop at the first match as basis is sorted by premise length\n",
    "                break\n",
    "    accuracy = correct / float(len(common_words))\n",
    "    output_df = pd.DataFrame()\n",
    "    output_df['source'] = word_map.keys()\n",
    "    output_df['ground truth'] = [value['gt'] for value in word_map.values()]\n",
    "    output_df['pac output'] = [value['pac_output'] for value in word_map.\n",
    "                               values()]\n",
    "    display(output_df)\n",
    "    print(\"{}/{} correct inflections\".format(correct, len(common_words)))\n",
    "    return accuracy\n",
    "\n",
    "def operation(dataframe):\n",
    "    \"\"\"Returns the operation sequence most common in the dataframe\"\"\"\n",
    "    counter = {}\n",
    "    for i, r in dataframe.iterrows():\n",
    "        opn_seq = ' '.join(r['deleted'] + r['added'])\n",
    "        try:\n",
    "            counter[opn_seq] += 1\n",
    "        except KeyError:\n",
    "            counter[opn_seq] = 1\n",
    "    return max(counter.items(), key=operator.itemgetter(1))[0]\n",
    "\n",
    "def apply_operation(operation_sequence, word):\n",
    "    \"\"\"Applies operation sequence on the word\"\"\"\n",
    "    for opn in operation_sequence:\n",
    "        if opn.startswith('::'):\n",
    "            # delete operation\n",
    "            opn = opn[2:]\n",
    "            if opn == '':\n",
    "                continue\n",
    "            else:\n",
    "                word = delete(opn, word)\n",
    "        else:\n",
    "            # insert operation\n",
    "            word = insert(opn, word)\n",
    "    return word\n",
    "\n",
    "\n",
    "def delete(old, word, new='', occurrence=1):\n",
    "    \"\"\"Removes to_delete from the word\"\"\"\n",
    "    li = word.rsplit(old, occurrence)\n",
    "    return(new.join(li))\n",
    "\n",
    "def insert(to_insert, word):\n",
    "    \"\"\"Appends to_insert to the word\"\"\"\n",
    "    return(word + to_insert)\n",
    "\n",
    "\n",
    "def build_relations(data):\n",
    "    \"\"\"\n",
    "    Build attribute -- object (source-word -- operation) relations from processed training data\n",
    "    denote ::operation for delete operations. For eg. ::ना shows delete ना\n",
    "    \"\"\"\n",
    "    relations = []\n",
    "    data['deleted'] = data['deleted'].apply(lambda opns: ['::' + opn for opn in opns])\n",
    "    for i, r in data.iterrows():\n",
    "        attr = r['source']\n",
    "        objects = r['deleted'] + r['added']\n",
    "        for obj in objects:\n",
    "            relations.append((obj, attr))\n",
    "    return relations\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    accuracy = evaluate('data/train/english-train-medium', 'data/test/uncovered/english-uncovered-test')\n",
    "    print(\"Accuracy: {}%\".format(accuracy*100))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
