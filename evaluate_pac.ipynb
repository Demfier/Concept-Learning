{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computed upper neighbours of 1000 concepts 23334\n",
      "Computed upper neighbours of 2000 concepts 0\n",
      "Computed upper neighbours of 3000 concepts 0\n",
      "Computed upper neighbours of 4000 concepts 0\n",
      "Computed upper neighbours of 5000 concepts 0\n",
      "computing introduced objects and attributes for concept 0 of 5075\n",
      "computing introduced objects and attributes for concept 1000 of 5075\n",
      "computing introduced objects and attributes for concept 2000 of 5075\n",
      "computing introduced objects and attributes for concept 3000 of 5075\n",
      "computing introduced objects and attributes for concept 4000 of 5075\n",
      "computing introduced objects and attributes for concept 5000 of 5075\n",
      "Done with introduced objects and attributes\n",
      "Done computing lattice\n",
      "# equivalence queries: 7, # disrespect trigger: 0, # special trigger: 6, # weak triggers: 1\n",
      "Done computing canonical basis\n",
      "Total implications: 2\n",
      "\n",
      "Total UNIQUE conclusions: 1\n",
      "\n",
      "{'प्रेम करना': {'gt': 'प्रेम कर रही होती', 'pac_output': 'प्रेम करतेहैं'}, 'रहना': {'gt': 'रह रहे हैं', 'pac_output': 'रहतेहैं'}, 'नागरिक बनाना': {'gt': 'नागरिक बनाती थीं', 'pac_output': 'नागरिक बनातेहैं'}, 'चुसना': {'gt': 'चुस रहा होता', 'pac_output': 'चुसतेहैं'}, 'सीखना': {'gt': 'सीखी होती', 'pac_output': 'सीखतेहैं'}, 'पकाना': {'gt': 'पका रहे होते', 'pac_output': 'पकातेहैं'}, 'बन्द करना': {'gt': 'बन्द करती होती', 'pac_output': 'बन्द करतेहैं'}, 'चुनना': {'gt': 'चुनते हैं', 'pac_output': 'चुनतेहैं'}, 'सहमाना': {'gt': 'सहमा रही होती', 'pac_output': 'सहमातेहैं'}, 'पसीजना': {'gt': 'पसीजती', 'pac_output': 'पसीजतेहैं'}, 'लौटना': {'gt': 'लौटती थी', 'pac_output': 'लौटतेहैं'}, 'ना करना': {'gt': 'ना कर रही होती', 'pac_output': 'ना करतेहैं'}, 'थामना': {'gt': 'थाम रही होगी', 'pac_output': 'थामतेहैं'}, 'बनाना': {'gt': 'बनाए थे', 'pac_output': 'बनातेहैं'}, 'तैरना': {'gt': 'तैरी होती', 'pac_output': 'तैरतेहैं'}, 'समझना': {'gt': 'समझ रहा था', 'pac_output': 'समझतेहैं'}, 'चढ़ाना': {'gt': 'चढ़ा रहे थे', 'pac_output': 'चढ़ातेहैं'}, 'सहमना': {'gt': 'सहमा हो', 'pac_output': 'सहमतेहैं'}, 'सूखना': {'gt': 'सूखती होती', 'pac_output': 'सूखतेहैं'}, 'उठाना': {'gt': 'उठाा था', 'pac_output': 'उठातेहैं'}, 'पिटना': {'gt': 'पिटे होते', 'pac_output': 'पिटतेहैं'}, 'घुमाना': {'gt': 'घुमाते होंगे', 'pac_output': 'घुमातेहैं'}, 'इस्तेमाल करना': {'gt': 'इस्तेमाल किये थे', 'pac_output': 'इस्तेमाल करतेहैं'}, 'बैठना': {'gt': 'बैठता था', 'pac_output': 'बैठतेहैं'}, 'बिलबिलाना': {'gt': 'बिलबिलाई होती', 'pac_output': 'बिलबिलातेहैं'}, 'हल चलाना': {'gt': 'हल चलाए होते', 'pac_output': 'हल चलातेहैं'}, 'चोट करना': {'gt': 'चोट कर रही होती', 'pac_output': 'चोट करतेहैं'}, 'ऐश करना': {'gt': 'ऐश करती होती', 'pac_output': 'ऐश करतेहैं'}}\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Evaluation approach currently used in the script:\n",
    "\n",
    "1. Take the words from test dataset present in the training data too\n",
    "2. Find the approriate implication from the set of learnt implications\n",
    "3. Apply the most common set of operation from the premise of appropriate impl.\n",
    "4. Compare the resultant with the one given in test set.\n",
    "\"\"\"\n",
    "\n",
    "import copy\n",
    "import helper\n",
    "import operator\n",
    "import pandas as pd\n",
    "import concept_context as cn\n",
    "\n",
    "TRAIN_DIR = 'data/train/'\n",
    "DEV_DIR = 'data/dev/'\n",
    "COV_TEST_DIR = 'data/test/covered/'\n",
    "UNCOV_TEST_DIR = 'data/test/uncovered/'\n",
    "\n",
    "\n",
    "def evaluate(train_dir, test_dir):\n",
    "    \"\"\"\n",
    "    Parameters:\n",
    "    -----------------\n",
    "    train_dir (Str): Path to the training file\n",
    "    test_dir (Str): Path to the testing file\n",
    "    \"\"\"\n",
    "\n",
    "    # Load training and testing data into a dataframe\n",
    "    train_data = pd.read_csv(train_dir, sep='\\t', names=['source', 'target',\n",
    "                                                         'pos_info'])\n",
    "    test_data = pd.read_csv(test_dir, sep='\\t', names=['source', 'target',\n",
    "                                                         'pos_info'])\n",
    "\n",
    "    attribute_size = train_data['source'].size\n",
    "\n",
    "    test_data = pd.merge(train_data, test_data, how='inner', on=['source',\n",
    "                                                                'target'])\n",
    "    test_data.dropna(inplace=True)\n",
    "    common_words = test_data['source']\n",
    "\n",
    "    # process training data\n",
    "    train_data = train_data.apply(helper.iterLCS, axis=1)\n",
    "\n",
    "    relations = build_relations(train_data)\n",
    "\n",
    "    # Build the concept lattice\n",
    "    concepts = cn.formalConcepts(relations)\n",
    "    concepts.computeLattice()\n",
    "\n",
    "    # Find canonical basis\n",
    "    concepts.computeCanonicalBasis(epsilon=0.1, delta=0.1, basis_type='pac')\n",
    "\n",
    "    print(\"Total implications: {}\\n\".format(len(concepts.canonical_basis)))\n",
    "\n",
    "    unique_conclusions = []\n",
    "    for impl in copy.deepcopy(concepts.canonical_basis):\n",
    "        if len(impl.conclusion) == attribute_size or len(impl.premise) == 0\\\n",
    "            or impl.premise == impl.conclusion:\n",
    "            concepts.canonical_basis.remove(impl)\n",
    "            continue\n",
    "        unique_conclusions.append(frozenset(impl.conclusion))\n",
    "\n",
    "    print(\"Total UNIQUE conclusions: {}\\n\".format(len(set(unique_conclusions))))\n",
    "    concepts.canonical_basis = set(sorted(list(concepts.canonical_basis), reverse=True))\n",
    "\n",
    "    implId_opnSeq_map = {}\n",
    "    for idx, impl in enumerate(concepts.canonical_basis):\n",
    "        premise = train_data['source'].isin(impl.premise)\n",
    "        premise_data = train_data[premise]\n",
    "        implId_opnSeq_map[idx] = operation(premise_data)\n",
    "\n",
    "    word_map = {}\n",
    "    for word in common_words:\n",
    "        # gt => Ground Truth\n",
    "        word_map[word] = {'gt': test_data[test_data['source'] == word]['target'].iloc[0]}\n",
    "        for idx, impl in enumerate(concepts.canonical_basis):\n",
    "            # use conclusion as it contains elements of premise too\n",
    "            if word in impl.conclusion:\n",
    "                opn_seq = implId_opnSeq_map[idx].split(' ')\n",
    "                output = apply_operation(opn_seq, word)\n",
    "                word_map[word]['pac_output'] = output\n",
    "                # stop at the first match as basis is sorted by premise length\n",
    "                break\n",
    "    print(word_map)\n",
    "\n",
    "\n",
    "def operation(dataframe):\n",
    "    \"\"\"Returns the operation sequence most common in the dataframe\"\"\"\n",
    "    counter = {}\n",
    "    for i, r in dataframe.iterrows():\n",
    "        opn_seq = ' '.join(r['deleted'] + r['added'])\n",
    "        try:\n",
    "            counter[opn_seq] += 1\n",
    "        except KeyError:\n",
    "            counter[opn_seq] = 1\n",
    "    return max(counter.items(), key=operator.itemgetter(1))[0]\n",
    "\n",
    "def apply_operation(operation_sequence, word):\n",
    "    \"\"\"Applies operation sequence on the word\"\"\"\n",
    "    for opn in operation_sequence:\n",
    "        if opn.startswith('::'):\n",
    "            # delete operation\n",
    "            opn = opn[2:]\n",
    "            if opn == '':\n",
    "                continue\n",
    "            else:\n",
    "                word = delete(opn, word)\n",
    "        else:\n",
    "            # insert operation\n",
    "            word = insert(opn, word)\n",
    "    return word\n",
    "\n",
    "\n",
    "def delete(old, word, new='', occurrence=1):\n",
    "    \"\"\"Removes to_delete from the word\"\"\"\n",
    "    li = word.rsplit(old, occurrence)\n",
    "    return(new.join(li))\n",
    "\n",
    "def insert(to_insert, word):\n",
    "    \"\"\"Appends to_insert to the word\"\"\"\n",
    "    return(word + to_insert)\n",
    "\n",
    "\n",
    "def build_relations(data):\n",
    "    \"\"\"\n",
    "    Build attribute -- object (source-word -- operation) relations from processed training data\n",
    "    denote ::operation for delete operations. For eg. ::ना shows delete ना\n",
    "    \"\"\"\n",
    "    relations = []\n",
    "    data['deleted'] = data['deleted'].apply(lambda opns: ['::' + opn for opn in opns])\n",
    "    for i, r in data.iterrows():\n",
    "        attr = r['source']\n",
    "        objects = r['deleted'] + r['added']\n",
    "        for obj in objects:\n",
    "            relations.append((obj, attr))\n",
    "    return relations\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    evaluate('data/train/hindi-train-medium', 'data/test/uncovered/hindi-uncovered-test')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
